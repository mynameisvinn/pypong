{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-11-03 08:37:12,071] Making new env: Pong-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "# gamespace \n",
    "env = gym.make(\"Pong-v0\") # environment info\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_obs = 80 * 80\n",
    "n_classes = 4  # ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "learning_rate = 1e-3\n",
    "gamma = .99\n",
    "decay = 0.99              # decay rate for RMSProp gradients\n",
    "\n",
    "# tf operations\n",
    "def tf_discount_rewards(tf_r):\n",
    "    discount_f = lambda a, v: a*gamma + v;\n",
    "    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r,[True, False]))\n",
    "    tf_discounted_r = tf.reverse(tf_r_reverse,[True, False])\n",
    "    return tf_discounted_r\n",
    "\n",
    "# downsampling\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1    # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float32).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_raw = tf.placeholder(tf.float64, [None, 6400], name=\"input\")\n",
    "X_ = tf.cast(tf.reshape(X_raw, (-1, 80, 80, 1)), tf.float32)\n",
    "Y_ = tf.placeholder(tf.float32, [None, n_classes], name=\"action\")\n",
    "\n",
    "# tf reward processing (need tf_discounted_epr for policy gradient wizardry)\n",
    "tf_epr = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"tf_epr\")\n",
    "tf_discounted_epr = tf_discount_rewards(tf_epr)\n",
    "tf_mean, tf_variance= tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n",
    "tf_discounted_epr -= tf_mean\n",
    "tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)\n",
    "\n",
    "\n",
    "K = 4  # first convolutional layer output depth\n",
    "L = 8  # second convolutional layer output depth\n",
    "M = 12  # third convolutional layer\n",
    "N = 200  # fully connected layer\n",
    "\n",
    "# 3x3 patch, 1 input channel (because input image has 1 channel), K output channels\n",
    "W1 = tf.Variable(tf.truncated_normal([3, 3, 1, K], stddev=0.1))  \n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "\n",
    "# 3x3 patch, K input channel, L output channels\n",
    "W2 = tf.Variable(tf.truncated_normal([3, 3, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "\n",
    "# 3x3 patch, L input channel, M output channels\n",
    "W3 = tf.Variable(tf.truncated_normal([3, 3, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "# w4 is 20x20xM bc Y3 output is 20x20 with M channels\n",
    "W4 = tf.Variable(tf.truncated_normal([20 * 20 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([N, n_classes], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([n_classes])/n_classes)\n",
    "\n",
    "stride = 1  \n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X_, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)  # output is 80x80, stride refers to pixels skipped per convolution\n",
    "\n",
    "stride = 2\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)  # output is 40x40, or 80 / 2\n",
    "stride = 2\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)  # output is 20x20 or 80 / 2\n",
    "\n",
    "# since Y3 is 20x20 with M channels, we want to reshape Y3 to YY of shape (-1, 20 * 20 * M)\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 20 * 20 * M])\n",
    "\n",
    "# now that this is a flat 1d vector, we can feed it to a vanilla function\n",
    "z4 = tf.matmul(YY, W4) + B4\n",
    "Y4 = tf.nn.relu(z4)\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "loss = tf.losses.log_loss(labels=Y_, predictions=Y, weights=tf_discounted_epr)\n",
    "op = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-562258c4a772>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-562258c4a772>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    print 'ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward)\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "prev_x = None\n",
    "xs,rs,ys = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        cur_x = prepro(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(n_obs)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        # stochastically sample a policy from the network\n",
    "        aprob = sess.run(Y, feed_dict={X_raw: np.reshape(x, (1,-1))})[0]\n",
    "        action = np.random.choice(n_classes, p=aprob)\n",
    "        label = np.zeros_like(aprob)\n",
    "        label[action] = 1\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        # record game history\n",
    "        xs.append(x)\n",
    "        ys.append(label)\n",
    "        rs.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            # update running reward\n",
    "            running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "\n",
    "            # parameter update\n",
    "            rs_float = np.array(rs).astype(\"float32\")\n",
    "            _ = sess.run(op, feed_dict={X_raw: np.vstack(xs), tf_epr: np.vstack(rs_float), Y_: np.vstack(ys)})\n",
    "\n",
    "            # print progress console\n",
    "            if episode_number % 10 == 0:\n",
    "                print ('ep {}: reward: {}, mean reward: {:3f}'.format(episode_number, reward_sum, running_reward))\n",
    "            else:\n",
    "                print ('\\tep {}: reward: {}'.format(episode_number, reward_sum))\n",
    "                \n",
    "                \n",
    "            # bookkeeping\n",
    "            xs,rs,ys = [],[],[] # reset game history\n",
    "            episode_number += 1 # the Next Episode\n",
    "            observation = env.reset() # reset env\n",
    "            reward_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
